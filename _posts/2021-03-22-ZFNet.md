---
layout: post
title: ZFNet
date: 2021-03-22
Author: JIUGE 
tags: [论文阅读]
comments: true
toc: false
---

这篇论文发表于2014年，作者对大型卷积神经各层的特征图进行了可视化分析。Net模型是在AlexNet基础上进行了微调。

<!-- more -->

# 论文简读：Visualizing and Understanding Convolutional Networks

论文信息：

![image-20210322211015631](https://gitee.com/changyv/md-pic/raw/master/20210322211017.png)

论文链接：https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf

发表时间：2014年，于ECCV。

## Abstract

大型卷积神经网络在以ImageNet数据集为benchmark的分类任务上表现卓越，但尚不清楚为什么会表现卓越，也不清楚如何来改进网络。针对这两个问题，作者提出了以一种可视化技术，用来深入了解特征层和分类器。另外，作者还做了一些消融(ablation)实验，以发现不同层对于模型性能的贡献。

## Introduction

自从LeCun在1990年提出LeNet，卷积神经网络在手写数字识别和面部识别上表现出卓越性能。(这篇论文发表于2013年)，而且在ImageNet数据集上，Krizhecsky的卷积网络实现了16.4%的错误率（第二名的错误率在26.1%）。作者将其归为以下因素：

+ 数量庞大的，已标注的训练数据集，
+ 强大的GUP算力，这使训练大型网络成为可能，
+ 更好的正则化策略：如Dropout。

卷积神经网络模型虽然取得了极大进展，但关于模型内部，或为何表现出良好的性能，仍然知之甚少。作者提出了一种可视化方法，作用如下：

+ 揭示各层特征的变化，
+ 观察，在训练过程中，特征图的变化，
+ 诊断出模型潜在的问题。

可视化技术采用了多层deconvnet（多层反卷积），用来将激活后的特征，投影回输入的像素空间中。作者通过遮挡输入图片的局部，来分析那些特征对分类器来说是重要的。

## 2 Approach

实验中，作者使用了全监督卷积网络模型AlexNet，模型中的一系列网络层将2维输入图像映射为向量$\hat{y}$，该向量表示C个类别可能的概率。网络的最后一层是softmax分类器。

**visualization with a Deconvnet**

作者提出一种方法：deconvnet（反卷积层），用来将激活映射回输入的像素空间：使用和卷积/池化层相同尺寸的卷积核/池化层，但做方向相反的卷积/池化操作。

反卷积层被添加到每个卷积层的后面，以提供从特征图返回图像像素的连续路径，见下图：

<img src="https://gitee.com/changyv/md-pic/raw/master/20210322200503.png" alt="image-20210322200454399" style="zoom:80%;" />

反卷积过程如下：解池化(unpooling)，激活，重建。

**unpooling**

卷积网络中，最大池化是不可逆的，所以作者记录了每个池化区域中最大值的位置。

**Rectification**

卷积层使用relu非线性激活函数，为了获得有效的重建特征（确保为正），作者继续使用relu函数来返回重构信息。

**Filtering**

作者使用和卷积层过滤器相同，但经过水平和垂直翻转的过滤器，对特征图进行反向卷积。

## 3 Training Details

作者使用和AlexNet近似的网络，区别于AlexNet将网络分为两部分，作者使用一个完整网络，用单GPU进行训练，其结构图如下：

![image-20210322202353533](https://gitee.com/changyv/md-pic/raw/master/20210322202355.png)

其它处理：

+ 使用ImageNet 2012训练数据集（1.3M张图像，1000个类别），
+ 对图像进行resize处理，
+ batch_size=128, momentum=0.9, 初始learning rate=0.01，
+ 第六、第七层全连接层后跟概率为0.5的dropout层。
+ 权重初始化为0.01，偏差初始化为0.

## 4 Convnet Visualization

作者给出的一些特征图可视化结果：

![image-20210322203837442](https://gitee.com/changyv/md-pic/raw/master/20210322203839.png)

作者还做了一些遮挡实验。详见论文4.2部分。

相关性分析：

+ 模型可能在隐式的计算，特征之间的联系，
+ 模型第5层显示出特征之间具有一定程度的对应关系。

## 5 Experiments

作者对模型进行了微调：第一层和第二层使用7\*7，stride=2卷积核，微调之后的模型，在ImageNet测试集上取得了14.7%的错误率。

接着，作者对AlexNet模型各个部分进行了分析：

+ 仅去掉第6，7层的全连接层，错误率略微上升，
+ 仅去掉中间2个卷积层，错误率变化了第一种情况相似，
+ 去掉中间的2个卷积层，同时去掉后面2个全连接层，模型性能下降明显，说明模型的整体深度极为重要。
+ 增加中间的卷积层，模型发生了过拟合。

## 6 Discussion

作者通过可视化分析，揭示了特征并不是随机产生，而是具有组成性。

通过遮挡实验，发现模型对于图像的局部结构高度敏感。

网络的整体深度，对于模型的性能来说，至关重要。













