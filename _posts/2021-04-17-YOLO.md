---
layout: post
title: YOLO V1
date: 2021-04-17
Author: liu_bai 
tags: [论文阅读]
comments: true
toc: false
---

这篇论文发表于2016年，于CVPR，是单检测器网络的开篇之作，之后的yolov2和tolov3在此基础上做了许多改进。

<!-- more -->

# You Only Look Once: Unified,  Real-Time Object Detection

<center>authors: Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi</center>

<center>constitute: University of Washington, Allen Institute for AI, Facebook AI Research</center>

<center>code: http://pjreddie.com/yolo/ </center>

这篇论文发表于2016年，于CVPR，是单检测器网络的开篇之作，之后的yolov2和tolov3在此基础上做了许多改进。

## Abstract

​	作者提出一种新的目标检测方法：YOLO。相比于RCNN系列使用分类器处理目标检测任务，YOLO将目标检测当作回归问题处理，且集成到了网络里，实现了端到端的训练优化。

YOLO检测速度：45FPS/秒，小版本的YOLO-tiny可以达到155FPS/秒。

## 1 Introduction

DPM(deformable parts models, DPM)使用移动滑窗，使检测器在整个图像上以均匀间隔运行检测。该方法是将图像分类应用到了目标检测上。

RCNN使用区域推荐网络(RPN)得到候选区域，然后使用分类器对这些候选区域进行分类，使用非极大值抑制(NMS)进行后处理，以消除重复检测，得到目标检测结果。

它们具有以下问题：

+ 分类器单独训练，

+ 网络不是一个整体，训练优化困难。

YOLO是一个整体网络，从图片输入到检测结果输出一步到位。实现了端到端的训练优化。相比于前者，速度快，训练优化简单。处理流程如下图。

<img src="C:%5CUsers%5Cjgang%5CDesktop%5Cpresentation%5C6-2.jpg" alt="image-20210417141115313" style="zoom: 60%;" />

## 2 Unified Detection

YOLO网络结构如下：

<img src="C:%5CUsers%5Cjgang%5CDesktop%5Cpresentation%5C6-3.jpg" alt="image-20210417141536936" style="zoom: 80%;" /> 

YOLO网络结构基于GoogLeNet。有24个卷积层，2个全连接层。Inception模块由3×3卷积层和1×1还原层（reduction layer）组成。网络最终输出 7\*7\*30的张量（tensor）。

YOLO设计细节：

1. YOLO将输入图像划分为S×S的网格（grid cell），如果目标中心落在某个网格内，则该网格负责预测这个目标。

2. 每个网格负责预测出B个边界框（bounding box）及其置信度（confidence scores）。置信度用于表示边界框是否有目标以及预测边界框的准确率。

   > 置信度等于$Pr(object) * IOU_{pred}^{truth}$ ，
   >
   > 其中，$pr(object)$ 取值为0或1，表示是否有目标，
   >
   > 使用IOU值来表示预测边界框的准确率。
   >
   > 如果没有目标，则置信度为0。

3. 预测出的边界框有5个参数值：$\{x,y,w,h,confidence\}$ ，(x,y)为预测边界框的中心点坐标，(w,h)是预测边界框的宽高，confidence为预测框的置信度。

4.  每个网格除了预测出B个边界框之外，还负责预测出C个条件概率：$Pr(class_i | object)$ ，C为类别数。这C个条件概率值与网格负责预测的B个边界框之间，是无关联的。

5. 测试的时候，将同一个网格预测出的条件概率向量和每个边界框置信度分别相乘：
   $$
   Pr(class_i | object) * Pr(object) * IOU_{pred}^{truth}=Pr(class)* IOU_{pred}^{truth}
   \tag{1}
   $$
    上述乘积结果表示预测出的边界框内，物体类别的置信度，以及预测出的边界框，与ground truth box的契合度。

于是，网络的最终输出就是一个$S*S*(B*5 + C)$ 的张量。

作者在PASCALVOC数据集上，使用S=7，B=2，由于有20个类别，所以网络输出为7×7×30的张量。

**Training**

在上述图3网络结构上，作者添加平均池化层和全连接测层，然后在ImageNet图像分类数据集上做预训练，在ImageNet2012验证集上是达到了88%的准确率。

预训练后，去掉添加的层，然后作者在网络末添加了4个卷积层和2个全连接层（新添加的层，使用随机初始化）。

一些处理：

+ 为了增加视觉信息，作者增加输入图片的尺寸：从224×224增加到448×448。

+ 网络最后一层负责预测类别概率和边界框坐标。所以作者对图片宽高进行归一化处理，使预测框中心坐标(x,y)相对于网络位置。

+ 网络最后一层使用线性激活函数，其余所有层使用leaky修正线性单元（leaky rectified linear activation）激活函数：
  $$
  \phi(x)=
  \begin{cases}
  x, \;\;\;\;\;\;if \;x >0\\
  0.1x,\;\;otherwise
  \end{cases}
  \tag{2}
  $$

+ 优化器使用均方和误差，原因是简单。。。

虽然优化起来简单，但均分根误差难以很好的平衡分类误差和定位误差。而且大多数网格是没有目标的，这些大量的低置信度，将主导梯度，导致模型不稳定，使训练过早的发散（diverge）。

为了解决上述问题，作者使用了两个超参数：$\lambda_{coord}=5 和\lambda_{noobj}=0.5$，以增加来自预测边界框的损失，减少来自边界框置信度（0或1）的损失。

而且，为了突出小边界框中偏差的重要程度，作者使用高和宽的平方根值来代替高和宽。

于是，训练过程中使用的损失函数就被定义为：

<img src="C:%5CUsers%5Cjgang%5CDesktop%5Cpresentation%5C6-4.jpg" alt="image-20201008221523574" style="zoom: 30%;" />

其中$Ⅱ_{i}^{obj}$ 表示目标是否落在第i个网格中。$Ⅱ_{ij}^{obj}$ 表示第i个网格的第j个边界框预测器负责预测该目标。

训练使用了135个epoch，期间学习率一直在做调整。

**Limitations of YOLO**

YOLO缺点：

+ 对小目标的检测（鸟群）会出现边界框争抢。因为每个网格只负责预测2个边界框，每个网格只属于一个类别。
+ 对输入图像的多次小采样，导致定位粗糙。
+ 损失函数对大小预测边界框的处理是相同的，但小预测边界框大多数是良性的（可能是使用IOU作为边界框置信度的原因）。模型主要错误在于定位不准确。

## 4 Experiment

接下来就是一些mAP和fps数据，优于两阶段检测器。

一张yolov1的网络结构结果图：

```
 layer   filters  size/strd(dil)      input                output
   0 conv     64       7 x 7/ 2    448 x 448 x   3 ->  224 x 224 x  64 0.944 BF
   1 max                2x 2/ 2    224 x 224 x  64 ->  112 x 112 x  64 0.003 BF
   2 conv    192       3 x 3/ 1    112 x 112 x  64 ->  112 x 112 x 192 2.775 BF
   3 max                2x 2/ 2    112 x 112 x 192 ->   56 x  56 x 192 0.002 BF
   4 conv    128       1 x 1/ 1     56 x  56 x 192 ->   56 x  56 x 128 0.154 BF
   5 conv    256       3 x 3/ 1     56 x  56 x 128 ->   56 x  56 x 256 1.850 BF
   6 conv    256       1 x 1/ 1     56 x  56 x 256 ->   56 x  56 x 256 0.411 BF
   7 conv    512       3 x 3/ 1     56 x  56 x 256 ->   56 x  56 x 512 7.399 BF
   8 max                2x 2/ 2     56 x  56 x 512 ->   28 x  28 x 512 0.002 BF
   9 conv    256       1 x 1/ 1     28 x  28 x 512 ->   28 x  28 x 256 0.206 BF
  10 conv    512       3 x 3/ 1     28 x  28 x 256 ->   28 x  28 x 512 1.850 BF
  11 conv    256       1 x 1/ 1     28 x  28 x 512 ->   28 x  28 x 256 0.206 BF
  12 conv    512       3 x 3/ 1     28 x  28 x 256 ->   28 x  28 x 512 1.850 BF
  13 conv    256       1 x 1/ 1     28 x  28 x 512 ->   28 x  28 x 256 0.206 BF
  14 conv    512       3 x 3/ 1     28 x  28 x 256 ->   28 x  28 x 512 1.850 BF
  15 conv    256       1 x 1/ 1     28 x  28 x 512 ->   28 x  28 x 256 0.206 BF
  16 conv    512       3 x 3/ 1     28 x  28 x 256 ->   28 x  28 x 512 1.850 BF
  17 conv    512       1 x 1/ 1     28 x  28 x 512 ->   28 x  28 x 512 0.411 BF
  18 conv   1024       3 x 3/ 1     28 x  28 x 512 ->   28 x  28 x1024 7.399 BF
  19 max                2x 2/ 2     28 x  28 x1024 ->   14 x  14 x1024 0.001 BF
  20 conv    512       1 x 1/ 1     14 x  14 x1024 ->   14 x  14 x 512 0.206 BF
  21 conv   1024       3 x 3/ 1     14 x  14 x 512 ->   14 x  14 x1024 1.850 BF
  22 conv    512       1 x 1/ 1     14 x  14 x1024 ->   14 x  14 x 512 0.206 BF
  23 conv   1024       3 x 3/ 1     14 x  14 x 512 ->   14 x  14 x1024 1.850 BF
  24 conv   1024       3 x 3/ 1     14 x  14 x1024 ->   14 x  14 x1024 3.699 BF
  25 conv   1024       3 x 3/ 2     14 x  14 x1024 ->    7 x   7 x1024 0.925 BF
  26 conv   1024       3 x 3/ 1      7 x   7 x1024 ->    7 x   7 x1024 0.925 BF
  27 conv   1024       3 x 3/ 1      7 x   7 x1024 ->    7 x   7 x1024 0.925 BF
  28 Local Layer: 7 x 7 x 1024 image, 256 filters -> 7 x 7 x 256 image
  29 dropout    p = 0.500        12544  ->   12544
  30 connected                            12544  ->  1715
  31 Detection Layer
```



