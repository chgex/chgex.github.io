---
layout: post
title: NiN
date: 2021-03-19
Author: JIUGE 
tags: [论文阅读]
comments: true
toc: false
---

这篇论文发表于2014年，提出了一种新的网络NIN，在CIFAR-10、CIFAR-100上取得了当时的SOTA。

<!-- more -->

# 论文简读：Network In Network

论文信息：

<img src="https://gitee.com/changyv/md-pic/raw/master/20210319204756.png" alt="image-20210319204752793" style="zoom:75%;" />

论文链接：https://arxiv.org/pdf/1312.4400.pdf

发表时间：2014年，于ICLR

## Abstract

作者提出了一种网络结构：Network In Network, NIN，用于增强模型对感受野内局部信息的识别。传统的卷积层使用线性过滤器后接非线性激活函数的方式，来提取特征信息，但作者提出了一种微型网络结构，来达到相似的效果。微型网络作用类似于卷积神经网络：它在输入上滑动，来获取特征信息。得到的特征信息将作为下一层的输入。

通过堆积这种微型网络结构，可以搭建出更深的网络。

使用微型网络和全局平均池化来代替全连接层，能较好的解决仅使用全连接层而引起的过拟合问题。

该网络模型(NIN)，在CIFAR-10和CIFAR-100数据集上获得了最优表现。

## 1 Introduction

卷积神经网络由卷积层和池化层构成。卷积层：线性滤波器(linear filter，等价于卷积核，kernel)， 与感受野做内积(inner product)运算，然后使用激活函数。最后的输出叫做特征图(feature map)。

作者将这个微型网络结构叫做mlpconv，如下图所示：

![image-20210319161845502](https://gitee.com/changyv/md-pic/raw/master/20210319161854.png)

> 左图是线性卷积层，右图是mplconv。

两者都在输入的局部感受野(local receptive field)上抽取特征信息。mlpconv使用了多个全连接层和非线性激活函数。NIN网络由多个mlpconv所组成。

传统卷积神经网络的分类使用的是全连接层，但作者在最后一个mlpconv层输出的特征图上，使用全局平均池化，将得到的向量传向softmax层，直接得到最后的结果。作者给出了以下理由：

+ 传统的卷积神经网络中，全连接层充当了黑盒子，导致无从得知损失是如何反传回卷积层的。
+ 相比全连接层，全局平均池化层具有可解释性。
+ 全局平均池化层加强了特征图和类别输出之间的联系。
+ 全连积层更易于发生过拟合，而且严重依赖于dropout 正则化。而全局平均池化层本身就是一个结构正则化器，可以防止过拟合。

## 3 Network In Network

NIN网络的2个核心部件：

+ MLP卷积层，
+ 全局平均池化。

**MLP Convolution Layers**

径向基网络(radial basis network)和多层感知器(multilayer perceptron)是两种常用的函数逼近器。作者选择多层感知器做作为网络的逼近器，给出的原因如下：

+ 多层感知器和卷积神经网络的机构相兼容，使用反向传播方式训练网络。
+ 多层感知器本身可以是深读模型，能够特征重用。

> MLP和RBN，具体细节暂时跳过。

作者使用MLP(multilayer perceptron，多层感知机)代替GLM(generalized linear model，广义线性模型)。MLP被用来和输入做卷积运算。线性卷积层与mlp卷积层的区别，如下图所示：

<img src="https://gitee.com/changyv/md-pic/raw/master/20210319191412.png" alt="image-20210319191410025" style="zoom:50%;" />

mlp卷积层计算方式如下：
$$
f_{i,j,k_1}^1=max({w_{k_1}^1}^T x_{i,j} + b_{k_1},0)\\
...\\
f_{i,j,k_n}^n=max({w_{k_n}^n}^T f_{i,j}^{n-1} + b_{k_n},0)\\
\tag{2}
$$

> n是多层感知器的层数。

作者使用ReLU(rectified linearunit, ReLU, 修正线单元)作为激活函数。

作者将mlp与maxout做了比较，暂时跳过。

> maxout类似一种激活函数，但相比于MLP，maxout会造成参数的多倍增加。

**Global Average Pooling**

传统的卷积神经网络在低层使用卷积，在分类时，最后一个卷积层的特征图被向量化，经过softmax逻辑回归层之后，作为全连接层的输入。上述结构桥接神经网络的分类器和卷积部分。卷积层用来提取特征，使用传统的方法，做结果分类。

全连接层容易过拟合，丢弃法(Dropout)是一种很好的解决方法，在训练中，丢弃法将全连接层一般的激活连接断开（参数设置为0）。

文中 ，作者提出了另外一种策略：全局平均池化，来代替卷积神经网络中的全连接层。作者指出了几个全局平均池化相比于全连接层的的优势：

+ 加强特征映射和类别之间的对应关系，且更适于卷积层，
+ 卷据池化层无需参数优化，
+ 全局平均池化层汇合了空间信息，所以对输入的空间变换更具鲁棒性。

**Network In Network Structure**

NIN网络是由多个mlpconv组合而成，网络顶端是全局平均池化层，下采样层位于mlp卷积层合maxout网络之间。下图为3个mlpconv层组成的NIN网络，每个mlpconv层有3层感知器。NIN的层数合微型网络的层数是可调整的。

<img src="https://gitee.com/changyv/md-pic/raw/master/20210319204538.png" alt="image-20210319201258420" style="zoom:45%;" />

4 Experiments

作者在4个benckmark上评估了模型的性能：CIFAR-10, CIFIAR-100, SVHM, MNIST。

关于网络结构：

+ 包含3个mlpconv，每个mlpconv后面使用一个最大池化层，用来对输入做下采样。
+ 除了最后一个mlpconv之外，其余的都使用了dropout，作为正则化器。
+ 网络顶部，使用全局平均池化，代替全连接层。
+ 作者使用的另一个正则化器是weights decay。

CIFAR-10

表现结果如下：

<img src="https://gitee.com/changyv/md-pic/raw/master/20210319204524.png" alt="image-20210319202800321" style="zoom:45%;" />

作者测试了使用dropout和不使用dropout的网络，在训练过程中的训练误差和测试误差变化情况。

<img src="https://gitee.com/changyv/md-pic/raw/master/20210319204513.png" alt="image-20210319202843785" style="zoom:45%;" />

此外，作者还在CIFAR-100、SVHM、MNIST数据集上测试了网络的表现性能，此处省略。

作者对使用全局平均池化和使用全连接层的网络，测试误差对比结果图：

<img src="https://gitee.com/changyv/md-pic/raw/master/20210319204501.png" alt="image-20210319203437509" style="zoom:50%;" />

以下是NIN网络最后一个mlpconv层特征图的可视化：

<img src="https://gitee.com/changyv/md-pic/raw/master/20210319204135.png" style="zoom: 33%;" />

总结

+ 作者提出了一种执行分类的网络：NIN，
+ 使用全局平均池化层（作为正则化器）代替全连接层，
+ NIN在CIFAR-10/100，SVHM达到了当时的SOTA。



基于pytorch的NIN实现：

```python
import torch
from torch import nn,optim
import torchvision
import torch.nn.functional as F 
import time

device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')


num_classes=10
lr,num_epochs=0.002,5


def nin_block(in_channels,out_channels,kernel_size,stride,padding):
    blk=nn.Sequential(
        nn.Conv2d(in_channels,out_channels,kernel_size,stride,padding),
        nn.ReLU(),
        nn.Conv2d(out_channels,out_channels,kernel_size=1),
        nn.ReLU(),
        nn.Conv2d(out_channels,out_channels,kernel_size=1),
        nn.ReLU()
    )
    return blk 


class GlobalAvgPool2d(nn.Module):
    def __init__(self):
        super(GlobalAvgPool2d,self).__init__()
    def forward(self,x):
        # 全局池化层的大小，等于输入的高和宽
        return F.avg_pool2d(x,kernel_size=x.size()[2:])
    
    
def NiN():
    net=nn.Sequential(
        nin_block(1,96,kernel_size=11,stride=4,padding=0),
        nn.MaxPool2d(kernel_size=3,stride=2),
        nin_block(96,256,kernel_size=5,stride=1,padding=2),
        nn.MaxPool2d(kernel_size=3,stride=2),
        nin_block(256,384,kernel_size=3,stride=1,padding=1),
        nn.MaxPool2d(kernel_size=3,stride=2),
        nn.Dropout(0.5),
        # 10类
        nin_block(384,10,kernel_size=3,stride=1,padding=1),
        GlobalAvgPool2d(),
        # 将4维输出，转为2维输出，
        # 其大小为(批量，10)
        d2l.FlattenLayer()
    )
    return net

# test
net=NiN()
X=torch.rand(1,1,224,224)
for name,blk in net.named_children():
    X=blk(X)
    print(name,'output shape:',X.shape)
   



# 图像预处理
# 构建一个列表：将多个图像变换实例集合到一起
resize=227
from torchvision import transforms
transform = transforms.Compose([
    transforms.Resize(size=resize),
    transforms.ToTensor(),
  	# Normalize 这8个值是针对 CIFAR-10 这个数据集算出来的，对于其他数据集不适用
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))
])

# 加载数据集
# CIFAR10 dataset
train_dataset = torchvision.datasets.CIFAR10(
    root='../datasets/', 
    train=True, 
    download=False, 
    transform=transform)

test_dataset= torchvision.datasets.CIFAR10(
    root='../datasets/', 
    train=False,
    download=False, 
    transform=transform)

batch_size=128


# data loader
train_iter=torch.utils.data.DataLoader(
    train_dataset, 
    batch_size=batch_size, 
    shuffle=True, 
    num_workers=4)
test_iter=torch.utils.data.DataLoader(
    test_dataset, 
    batch_size=batch_size, 
    shuffle=False, 
    num_workers=4)


def evaluate_accuracy(data_iter,net,device=None):
    # gpu
    if device is None and isinstance(net,torch.nn.Module):
        # 如果没有指定device，则使用net的device
        device=list(net.parameters())[0].device
    # 准确率，总数
    acc_sum,n=0.0,0
    # with torch.no_grad： disables tracking of gradients in autograd. 
    # model.eval()： changes the forward() behaviour of the module it is called upon.
    with torch.no_grad():
        for X,y in data_iter:
            if isinstance(net,torch.nn.Module):
                # 评估模式，该模式会关闭dropout
                net.eval()
                # torch.argmax(input, dim, keepdim=False) → LongTensor返回指定维度的最大值的索引。
                acc_sum+=( net(X.to(device)).argmax(dim=1) == y.to(device) ).float().sum().cpu().item()
            else: # 无GPU
                if('is_training' in net.__code__.co_varnames): # 如果有is_training这个参数
                    # 将is_training设置成False
                    acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() 
                else:
                    acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() 
            n+=y.shape[0]
    return acc_sum/n

def train(net,train_iter,test_iter,batch_size,optimizer,device,num_epochs):
    net=net.to(device)
    print('training on ',device)
    # 损失函数，使用交叉熵损失函数
    loss=torch.nn.CrossEntropyLoss()
    
    for epoch in range(num_epochs):
        train_l_sum,train_acc_sum,n,batch_count,start=0.0,0.0,0,0,time.time()
        for i,(X,y) in enumerate(train_iter):
            X=X.to(device)
            y=y.to(device)
            y_hat=net(X)
            # 计算损失
            l=loss(y_hat,y)
            # 梯度清零
            optimizer.zero_grad()
            # 反向传播
            l.backward()
            # 更新参数
            optimizer.step()
            
            print('epoch %d/%d, iter %d/391, loss %.3f' % (epoch,num_epochs,i,l.cpu().item()))


            # 更新损失和正确率
            train_l_sum+=l.cpu().item()
            train_acc_sum+=(y_hat.argmax(dim=1) == y ).sum().cpu().item()
            n+=y.shape[0]
            batch_count+=1
        # 测试集上的正确率
        test_acc=evaluate_accuracy(test_iter,net)
        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'
        %(epoch+1,train_l_sum/batch_count,train_acc_sum/n,test_acc,time.time()-start))    

optimizer=torch.optim.Adam(net.parameters(),lr=lr) 

d2l.train(net,train_iter,test_iter,batch_size,optimizer,device,num_epochs)

```









