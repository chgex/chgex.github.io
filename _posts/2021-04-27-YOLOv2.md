---
layout: post
title: YOLO v2
date: 2021-04-27
Author: liu_bai 
tags: [论文阅读]
comments: true
toc: false
---

YOLOV2，发表于2016年。结合了批量正则化、anchor、多粒度特征等多个检测器设计。保持原有精度的基础上，速度和检测类别上有极大提升。

<!-- more -->

# YOLO 9000: Better,Faster,Stronger

<img src="D:%5CBlog%5Cphoto%5Cimage-20210426161346345.png" alt="image-20210426161346345" style="zoom:30%;" />

发表时间：2016年，于arvix，2017年正式接收于CVPR。

[论文链接](https://openaccess.thecvf.com/content_cvpr_2017/papers/Redmon_YOLO9000_Better_Faster_CVPR_2017_paper.pdf)

## Abstract

YOLO9000可以检测9000多个类别，在当时的PASCAL VOC和COCO数据集上取得了SOTA的效果：67FPs下，在VOC 2007上的mAP为76.8，40FPs下，mAP78.6%。作者还提出一个方法以实现同步训练目标检测和图像分类：使用该方法，实现YOLO9000在COCO数据集和ImageNet数据集上的同步(simultaneously)训练（就是将ImageNet图像分类数据集的图片，作为单目标预测，而不用考虑标注问题，以减少标注成本）。

## 1 Introduction

基于YOLO的改进YOLO9000，在ImageNet和coco上训练，可以检测 9000类物体。

## 2 Better

由于YOLO在召回率上低于基于区域推荐的方法，所以YOLOV2的改进主要是保持精确度的同时，提升召回率和改进定位精度上面。

YOLO V2结合了当前检测器许多优秀的设计思想，对于新能的改变，见下表2。

<img src="D:%5CBlog%5Cphoto%5Cimage-20210427084127158.png" alt="image-20210427084127158" style="zoom:150%;" />

**Batch Normalization**

在所有的卷积层后增加BN层，mAP提高了2%。在增加了BN层之后，去掉dropout层，模型也不会出现过拟合。

**High Resolution Classifier**

AlexNet以分辨率224\*224在图像分类数据集上做预训练，然后调整分辨率到448\*448，做为检测器正式训练。

在YOLOV2Hon中，作者将分类网络的分辨率固定在448\*448，做预训练，在之后的检测器训练时不再调整。这种高分辨率分类网络预训练，使mAP提升了4%。

**Convolutional With Anchor Boxes**

YOLOV1中，顶层卷积特征提取器之后的全连接层负责边界框坐标预测。Faster RCNN使用RPN预测anchor box的坐标和置信度。

作者将anchor设计结合到YOLO中：去掉全连接测层，使用anchor box来预测边界框。YOLO的全卷积层使用factor为32的下采样，对于426\*416的输入图像，得到13\*13的特征图。

增加anchor之后，YOLO的召回率从81%增加到了88%，但mAP从69.5下降到了69.2。

**Dimension Cluster**

将anchor引入YOLO，面临两个问题：

1. anchor的设计。为了避免手动设计anchor对网络造成影响，作者使用K-means聚类算法（针对训练集边界框），得到k=5类anchor，细高的box占了大多数，这与手动设计的anchor具有差异。

   > k-means聚类算法使用欧式距离，但用在边界框距离计算上就不合适，所以作者使用$d(box,centroid)=1-IOU(box,centroid)$代替欧式距离。 

**Direct location prediction**

引入anchor遇到的第二个问题：模型不稳定问题。大都数来自于预测的Box的中心坐标$(x,y)$ 。

区域推荐网络（FPN）输出一个预测值$t_x,t_y$ ，于是中心坐标$(x,y)$ 计算为：
$$
x=(t_x * w_a)-x_a\\
y=(t_y * h_a)-y_a
$$

> 上述下标为a的应该是anchor的尺寸？

如果给出的预测值$t_x=1$，则box会右移anchor box的宽度。$t_x=-1$右移同理。如果不做限制，这些anchor box会出现在图像的任意点。作者使用logistic activation限制网络的预测值，落在0-1的范围内：
$$
b_x=\sigma(t_x)+c_x\\
b_y= \sigma(t_y)+c_y\\
b_w = p_w e^{t_w}\\
b_h = p_h e^{t_h}\\
Pr(object) * IOU(b,object)=\sigma(t_o)
$$
使用聚类和直接预测边界框中心位置，使mAP提升了5%。

**Fine-Grained Features**

多粒度特征：passthrough层将高分辨率和低分辨率特征图连结，将临近特征添加到不同的channel，而不是可见位置上，类似于ResNet。这使26\*26\*512特征图转为13\*13\*2048的特征图。

添加passthrough层，模型mAP提升了1%。

**Multi-Scale Training**

YOLOV2训练过程中，使用不同的分辨率的图像：320\*320到608\*608，resize网络到该维度，然后继续迭代训练。

低分辨率228\*228，90FPs时，mAP与Faster rcnn相同。高分辨率下，在VOC2007上mAP为78.6%。细节数据如下表。

## 3 Faster

大多数检测模型，依赖VGG-16网络提取特征，虽然高效但参数量巨大：检测一张224\*224的图像需经过306.9亿次浮点运算​。

于是作者自己设计了一个分类模型:Darknet-19，细节如下表：

<img src="D:%5CBlog%5Cphoto%5Cimage-20210427103344069.png" alt="image-20210427103344069" style="zoom:50%;" />

Darknet-19有19个卷积层，5个最大池化层。一次图片检测只需要55.8亿次运算。在ImageNet上实现了72.9%的top-5准确率（top1准确率为91.2%）。

**Training for classification**

预训练：

+ 在ImageNet 1000类数据集上训练了160个epoch；
+ 初始学习率为0.1，polynomial rate decay=4，weights decay=0.0005，momentum=0.9
+ 随机梯度下降优化；
+ 数据增强：随机剪裁，旋转，色调饱和度曝光度偏移。

训练：

+ 修改预训练网络：
  + 去掉最后一层卷积层，
  + 增加：3个 3\*3的卷积层，1\*1卷积层（在每个卷积层后面），1个1024维的过滤器。
  + 增加passthrough层，以使用粒度特征。

4 Stronger

**Hierarchical classification**

为了检测多个类别，作者设计了一个wordtree hierarchical，如下图：

<img src="D:%5CBlog%5Cphoto%5Cimage-20210427105215207.png" alt="image-20210427105215207" style="zoom:50%;" />

## 5 Conclusion

+ 提出YOLO9000，实时检测器；
+ 提出wordtree，为ImageNet图像分类数据集提供更多的分类；



Reference

[1] https://blog.csdn.net/wfei101/article/details/78944891

[2] PPT, [deepsystems.io:Illustration of YOLO](https://docs.google.com/presentation/d/1aeRvtKG21KHdD5lg6Hgyhx5rPq_ZOsGjG5rJ1HP7BbA/pub?start=false&loop=false&delayms=3000&slide=id.p)

[3] https://blog.csdn.net/litt1e/article/details/88852745



