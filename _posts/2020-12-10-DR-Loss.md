---
layout: post
title: DR Loss Improving Object Detection by Distributional Ranking
date: 2020-12-10
Author: JIUGE 
tags: [论文阅读]
comments: true
toc: false
---

这篇论文提出一种损失函数focal loss，以解决类间不平衡问题。即每次考虑一个样本对，根据置信度排序，将分类问题转换为排序问题。

使用DR Loss代替RetinaNet 的focal loss，仅改变损失函数，COCO上mAP就能从39.1%提高到41.7%。

<!-- more -->

# 论文简读 DR Loss: Improving Object Detection by Distributional Ranking



![](https://gitee.com/changyv/md-pic/raw/master/20201209225332.jpg)

论文链接：https://arxiv.org/abs/1907.10156

## Abstract

单阶段检测器旨在在同一阶段中，从所有候选对象中识别出前景对象，这种网络结构容易受到来自两方面不平衡问题的影响：

1. 前景候选对象(fornground candidates)和背景候选对象(background candidates)之间，类间不平衡(inter-class imbalance)问题；
2. background candidates之间，类内不平衡(intra-class imbalance)问题，因为背景候选中只有少数候选对象是难以识别的。

针对以上问题，提出了distributional ranking loss(DR Loss)：将分类问题转化为置信度排序问题，以解决类间不平衡问题。每次考虑图像中的样本对（包含一个正样本和一个负样本）；然后根据前景框和背景框的置信度作决策；最后对样本置信度分布进行优化。

使用DR Loss代替RetinaNet 的focal loss，并以ResNet-101为骨干网络，仅仅改变损失函数，就可以将COCO单尺度试验的mAP从39.1%提高到41.7%。

## 1.Introduction

两阶段检测器有优秀的性能，但在实际中面临算力问题的影响，单阶段检测器直接从所有候选对象中选择出前景对象，这种处理方式直接有效，但容易受到类别不均衡问题的影响。

论文提出一种图像依赖的排序损失函数，以处理不平衡问题。首先，为了解决类内（候选框数量）不平衡问题，将分类问题转化为一个排序问题。将一个背景框和一个前景框作为排序对，以此解决类间不平衡问题。其次，针对类间不平衡问题，使用DR Loss对前景框和背景框的置信度分布优化。如下图：

![](D:%5Cblog%5Ctypora_photo%5C20201205_DR_5.jpg)

> 左图是候选框的原始分布；
>
> 为了平衡候选框的数量，对候选框的置信度排序，并每次采样样本对（包含一个正样本和一个负样本）；子图(2)表示对原始分布进行调整；子图(3)表示引入间隔后原始分布的排序。

## 3.DR Loss

给出一张图片的候选边界框集合，检测器分类模块从背景对象中识别出前景对象。令$\theta$ 表示分类器，它通过以下优化方式习得：
$$
\min_\theta\sum_{i}^{N}\sum\ell(p_{i,j,k})
\tag{1}
$$
N表示总图片数量；$p_{i,j,k}$表示第i张图片中第j个候选对象属于第k类的概率，是由分类器$\theta$ 预测出的；$\ell(\cdot)$ 为损失函数。大多数分类器都是通过最小化交叉熵损失函数(cross entropy loss)或方差(variants)进行学习。

但式(1)容易受到类间不平衡问题的影响。于是将上式(1)改写为：
$$
\min_\theta\sum_{i}^{N}(\sum_{j_+}^{n+}\ell(p_{i,j_+})+\sum_{j_-}^{n_-}\ell(p_{i,j_-}))
\tag{2}
$$
其中，$j_+，j_-$分别表示第j个正样本和负样本；$n_+，n_-$ 分别表示正负样本的个数。当负样本数远远大于正样本数时，式子中的后一项将占据主导，这将导致负样本对损失的贡献远远超过正样本。常用的解决方式是增大正样本的损失权重。论文提出了一种解决方法：

### 3.1.Ranking

为了减少类间不平衡问题，我们优化正负样本之间的排序。对于一个样本对(正负样本各一个)，理想的模型是使用一个大间隔$\gamma$ 将正样本排在负样本之前：
$$
\forall j_+,j_-,p_{j_+}-p_{j_-}\geq\gamma
$$

> $\gamma$是非负常量，表示间隔。

该排序模型很好的优化了单个正负样本之间的关系。于是对单个图像的排序目标，就可以写成：

$$
\min_\theta\sum_{j_+}^{n_+}\sum_{j_-}^{n_-}\ell(p_{j_-}-p_{j_+}+\gamma)
\tag{3}
$$
其中，损失函数$\ell(\cdot)$ 使用了hinge loss(铰链损失)：
$$
\ell_{hinge}(z)=[z]_+=
\begin{cases}
z, \space z>0    \\
0,\space o.w.
\end{cases}
$$
式(3)进一步等价于：
$$
\frac{1}{n_+n_-}
\sum_{j_+}^{n_+}\sum_{j_-}^{n_-}\ell(p_{j_-}-p_{j_+}+\gamma)\\
=E_{j_+,j_-[\ell(p_{j_-}-p_{j_+}+\gamma)]}
\\
\tag{4}
$$
上式表示最小化分类损失等价于最小化随机采样对排序损失的期望。

排序损失通过比较每个正负样本的排名，解决了类间不平衡问题。

然而，却忽略了：负样本中的难分样本也是不平衡的，除此之外，还有排序问题会引发大量的配对问题。

针对这些问题，引入了Distributional Ranking：

### 3.2.Distributional Ranking

在检测任务中，大多数负样本能很好的排序，这是因为随机的样本对不会造成显著的排序损失。因此，在(3)式上，我们优化排序边界来避免琐碎解：
$$
\min_\theta\ell(\max_{j_-}\{p_{j_-}\}-\min_{j_+}\{p_{j_+}\}+\gamma)
\tag{5}
$$
如果能将最低置信度的正样本排在最高置信度的负样本之前，那么图片中的整个样本集就被很好的排序。上式(5)中的样本对就表示了这种最糟糕情况，式(5)的情况能获得最大的排序损失。和传统排序损失相比，从最糟糕情况开始来优化损失，将更加有效，因为这能使得配对数量从$n_+n_-$减少到1，并且有效的减少了类间不平衡问题。但是，这种形式对挑选出的样本对非常敏感(因为是取的最大与最小而组成的一对)，可能导致检测模型的退化。

为了增加鲁棒性，引入正负样本置信度分布，并通过以下式子计算期望：
$$
P_+=\sum_{j_+}^{n_+}q_{j_+}p_{j_+};\\
P_-=\sum_{j_-}^{n_-}q_{j_-}p_{j_-};\\
$$

> $q_+\in \Delta $和$q_-\in \Delta $ 分别表示正负样本的分布；
>
> $P_+$和$P_-$ 分别表示对应分布下的期望置信度。
>
> $\Delta=\{q:\sum_{j}q_j=1,\forall{j},q_j\geq0 \}$ 。
>
> 当$q_+$和$q_-$是均匀分布时，$P_+$和 $P_-$表示原始分布的期望。

根据以上定义，最差情形时，样本对的分布变为：
$$
P_+=\min_{q_+\in \Delta}\sum_{j_+}^{n_+}q_{j_+}p_{j_+};\\
P_-=\max_{q_-\in \Delta}\sum_{j_-}^{n_-}q_{j_-}p_{j_-};\\
$$
于是，式(5)可以写成：
$$
\min_{\theta}\ell(P_--P_++\gamma)
$$
上式表示正负样本在最差情形下的分布排序。

但上述模型还是不具有鲁棒性，因为实际产生的分布域是不受约束的。它将集中到简单样本，从而忽略包含大量信息的原始分布，因此，对分布做如下限制：
$$
P_-=\max_{q_-\in \Delta,\Omega(q_-||o_-)\leq{\epsilon}_-}\sum_{j_-}^{n_-}q_{j_-}p_{j_-};\\
-P_+=\max_{q_+\in \Delta,\Omega(q_+||o_+)\leq {\epsilon}_+}\sum_{j_+}^{n_+}q_{j_+}p_{j_+};
$$

> $o_+$和$o_-$ 分别表示正负样本的分布；
>
> $\Omega(\cdot)$ 是一个正则化器，
>
> $\epsilon_-$和$\epsilon_+$ 控制生成分布的自由度。

为了得到约束分布，我们考虑子问题：
$$
\max_{q_-\in \Delta}\sum q_{j_-}p_{j_-}\\
s.t.{\space}{\Omega(q_-||o_-)}\leq{\epsilon_-}
$$
根据对偶理论1(dual theory)，给定$\epsilon_-$，可以找到参数$\lambda_-$ ，通过以下方式来优化$q_- $ ：
$$
\max_{q_-\in \Delta}\sum q_{j_-}p_{j_-}-{\lambda\_}{\Omega(q_-||o_-)}
$$
式子中的前一项是关于$q_-$的线性，因此，如果$\Omega(\cdot)$是$q_-$的凸函数，则根据论文Convex optimization(Stephen .et, 2004)提出的方法可以有效求解。本文使用使用KL-散度正则化方法：

> KL散度用于衡量二者部分分布是否相近。

**Proposition1**，对于问题：
$$
\max_{q_-\in \Delta}\sum q_{j_-}p_{j_-}-{\lambda\_}{KL(q_-||o_-)}
$$


使用KKT条件求得近似解：
$$
q_{j_-}=\frac{1}{Z_-}o_{j_-}exp(\frac{p_{j_-}}{\lambda_-});\\
Z_-=\sum_{j_-}o_{j_-}exp(\frac{p_{j_-}}{\lambda_-})
$$
对于正样本分布，求得相应的解：
$$
q_{j_+}=\frac{1}{Z_+}o_{j_+}exp(\frac{p_{j_+}}{\lambda_+});\\
Z_+=\sum_{j_+}o_{j_+}exp(\frac{p_{j_+}}{\lambda_+})
$$
**Remark 1**

这些propositions表明，样本越难，样本的权重越大。此外，权重还会受到该图像中其它样本的影响。

原始分布($o_-和o_+$) 通过加权每个候选对象(weighting each candidate)也会影响到推导(derived)分布，因此，问题的先验知识可以编码到原始分布中，使生成的分布更流畅。因此我们用$o_-$为例来阐释不同的分布：

+ 均匀分布(uniform distribution)：$\forall j,o_{j-}=1/n_-$ 对于一个常量值，近似解可以被简化为：

$$
q_{j-}=\frac{1}{Z_-}exp(\frac{p_{j_-}}{\lambda _-});\\
Z_-=\sum_{j_-}exp(\frac{p_{j_-}}{\lambda_-})
$$

+ 难负样本挖掘(hard negative mining)：在这种场景下，

为了简化损失函数，论文采用了均匀分布(即将$o_+$和$o_-$ 简化为均匀分布1/n)，图2显示了分布变化情况：

<img src="D:%5Cblog%5Ctypora_photo%5C20201205_DR_2.jpg" style="zoom:30%;" />

有了分布的近似解，于是分布的期望可以通过以下式子计算：
$$
\hat{P_-}=\sum_{j_-}^{n_-}q_{j_-}p_{j_-}=\sum_{j_-}^{n_-}\frac{1}{Z_-}exp(\frac{p_{j_-}}{\lambda_-})p_{j_-}\\
\hat{P_+}=\sum_{j_+}^{n_+}q_{j_+}p_{j_+}=\sum_{j_+}^{n_+}\frac{1}{Z_+}exp(\frac{p_{j_+}}{\lambda_+})p_{j_+}\\
\tag{6}
$$
使用平滑近似替代铰链损失，常见的替代方式有平方损失和逻辑回归：
$$
{\ell}_{quad}(z)=
\begin{cases}
z,z\geq\rho\\
\frac{(z+\rho)^2}{4\rho},-\rho<z<\rho\\
0,z\leq-\rho
\end{cases}
\tag{7}
$$

$$
\ell_{logistic}(z)=\frac{1}{z}log(1+exp(Lz))\\
\tag{8}
$$

> $\rho$和L控制函数的近似误差；
>
> L越大，越接近铰链损失；
>
> $\rho$工作在相反的方向；

图3比较了铰链损失和不同方差时的近似损失：

<img src="D:%5Cblog%5Ctypora_photo%5C20201205_DR_3.jpg" style="zoom:45%;" />

将以上部分组合在一起，最终的DR Loss定义为：
$$
\min_{\theta}\mathcal{L}_{DR}(\theta)=\sum_{i}^{N}\ell_{logistic}(\hat{P}_{i,-}-\hat{P}_{i,+}+\lambda)
\tag{9}
$$

> $\hat{P}_{i,-}$在式(6)中给出；
>
> $\ell_{logistic}(\cdot)$在式(8)中给出；

如果图像中没有正样本，那就令$\hat{P}_{i,+}=1$ 。和传统的排序损失相比，对两个分布的期望进行排序，配对数从n^n缩小到1，这使的优化更加有效。

通过最小批量的标准SSD来优化DR Loss：
$$
\theta_{t+1}=\theta_t-{\eta}\frac{1}{m}\sum_{s=1}^{m}\bigtriangledown\ell_{t}^{s}
$$
> 梯度的范数被用于测量收敛性，这是非凸优化的标准准则。

### 3.3.Recover Classification from Ranking

将排序转换到分类，一个直接的方法是为所有的排序得分设置一个阈值，然而由于图片依赖(image-dependent)，不同图片的排序得分范围不一，所以需要校准后分类。正负样本的排序分数边界如下：
$$
\forall j_+,j_-,p_{j_+}-p_{j_-}\geq \gamma\\
$$
等价于：
$$
\forall j_+,p_{j_+}\geq \gamma;\space
\forall j_-,p_{j_-}\leq 1-\gamma
$$
于是，通过大间隔，就可以恢复分类。

最终的损失函数为：
$$
min\sum_{i}^{N}\tau\ell_{DR}^{i}+\ell_{Reg}^{i}
$$

> $\ell_{Reg}$ 是RetinaNet中的回归损失，不变；
>
> $\tau$ 是一个参数，用于平衡分类和回归之间的权重；
>
> 实验中，令$\tau=4$ ;

## 4.Experiments

### 4.2.Parameters in DR Loss

式子(9)中，DR Loss有3个参数：$\lambda_+,\lambda_-,L$ ，其中，$\lambda_+,\lambda_-$规范化正负样本置信度分布。$L$ 控制损失损失函数的平滑性。实验中，令$\gamma=0.5$ 。

相比于foal loss，DR Loss多了一个参数。但是RetinaNet缺乏对正负(样本置信度)分布关系的优化，这个额外的参数是用来初始化分类器的输出概率，来拟合背景对象分布。

### 4.3.Effet of Parameters

证明了$\lambda$的有效性；证明了平滑的有效性；证明了样本对的有效性；

证明了DR Loss的有效性；

### 4.4.Comparison with State-of-the-Art

将使用了DR Loss的RetinaNet记为Dr.Retina，在COCO测试集上比较：

![](D:%5Cblog%5Ctypora_photo%5C20201205_DR_4.jpg)  

Dr.Retina将mAP从39.1%提升到了41.7%，说明DR Loss比focal Loss更好的解决检测的不均衡问题。

## 5.Conclusion

本文提出DR Loss来解决单阶段目标检测的样本不均衡问题。第一次将分类问题转换为了排序问题，更好的平衡了正负样本。优化了原始分布，平衡了背景候选中的难分样本。coco上的实验验证了DR Loss 的有效性。

下面是官方给出的用PyTorch实现Sigmoid+DR Loss的源码：

```python
import torch
from torch import nn
import torch.nn.functional as F
import math


class SigmoidDRLoss(nn.Module):
	# 几个超参数，依次对应γ、λ、L(最后一个参数不清楚，它乘在损失函数前)
    def __init__(self, pos_lambda=1, neg_lambda=0.1/math.log(3.5), L=6., tau=4.):
        super(SigmoidDRLoss, self).__init__()
        self.margin = 0.5
        self.pos_lambda = pos_lambda
        self.neg_lambda = neg_lambda
        self.L = L
        self.tau = tau

    def forward(self, logits, targets):
    	# 标注相关信息
        num_classes = logits.shape[1]
        dtype = targets.dtype
        device = targets.device
        class_range = torch.arange(1, num_classes + 1, dtype=dtype, device=device).unsqueeze(0)
        t = targets.unsqueeze(1)
        # 获得正负样本id
        pos_ind = (t == class_range)
        neg_ind = (t != class_range) * (t >= 0)
        # 概率p使用sigmoid求得
        pos_prob = logits[pos_ind].sigmoid()
        neg_prob = logits[neg_ind].sigmoid()
        # 对应于式(3.)
        neg_q = F.softmax(neg_prob/self.neg_lambda, dim=0)
        neg_dist = torch.sum(neg_q * neg_prob)
        # 论文中提到，如果图像中没有正样本，则正样本的P使用1代替
        if pos_prob.numel() > 0:
        	# 对应于式(3.6)
            pos_q = F.softmax(-pos_prob/self.pos_lambda, dim=0)
            pos_dist = torch.sum(pos_q * pos_prob)
            # 对应于式(3.9)
            loss = self.tau*torch.log(1.+torch.exp(self.L*(neg_dist - pos_dist+self.margin)))/self.L
        else:
        	# 对应于式(3.9)
            loss = self.tau*torch.log(1.+torch.exp(self.L*(neg_dist - 1. + self.margin)))/self.L
        return loss

```

参考

Qian Q , Chen L , Li H , et al. DR Loss: Improving Object Detection by Distributional Ranking[C]// 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2020.

https://blog.csdn.net/Skies_/article/details/105599372

https://zhuanlan.zhihu.com/p/145927429

## 相关概念

正负样本：

参考：https://blog.csdn.net/clearch/article/details/80224223

> 正样本就是任务所要检测的目标物，比如在人脸识别中不同种族年龄的人脸、不同表情的人脸、戴不同装饰的人脸等等情况下的人脸；
>
> 而负样本则是目标物所处的不同背景(注意：此背景不包含人脸)，负样本就是这些不包含人脸的图片。

faster和SSD两种检测框架对于正负样本的选取规则：

> 检测问题中的正负样本是程序中（网络）生成出来的ROI，也就是faster rcnn中的anchor boxes以及SSD中在不同分辨率的feature map中的默认框，这些框中的一部分被选为正样本，一部分被选为负样本，另外一部分被当作背景或者不参与运算。
>
> 一个小技巧：在训练检测网络时，若已经训练出一个较好的检测器，在用它进行测试时，还会有一些误检，这时可以把误检的图像加入负样本中retrain检测网络，迭代次数越多则训练模型越好



focal loss

参考：https://blog.csdn.net/weixin_31866177/article/details/81155826

one-stage detector的准确率不如two-stage detector的原因，作者认为原因是：样本的类别不均衡导致的。

我们知道在object detection领域，一张图像可能生成成千上万的candidate locations，但是其中只有很少一部分是包含object的，这就带来了类别不均衡。

类别不均衡会导致：负样本数量太大，占总的loss的大部分，而且多是容易分类的，因此使得模型的优化方向并不是我们所希望的那样。

针对类别不均衡问题，作者提出一种新的损失函数：focal loss，这个损失函数是在标准交叉熵损失基础上修改得到的。**这个函数可以通过减少易分类样本的权重，使得模型在训练时更专注于难分类的样本**。

为了证明focal loss的有效性，作者设计了一个dense detector：RetinaNet，并且在训练时采用focal loss训练。实验证明RetinaNet不仅可以达到one-stage detector的速度，也能有two-stage detector的准确率。

以上。

